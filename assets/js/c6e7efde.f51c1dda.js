"use strict";(self.webpackChunkzio_site=self.webpackChunkzio_site||[]).push([[42332],{95788:(e,n,a)=>{a.d(n,{Iu:()=>d,yg:()=>g});var r=a(11504);function t(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach((function(n){t(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function l(e,n){if(null==e)return{};var a,r,t=function(e,n){if(null==e)return{};var a,r,t={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],n.indexOf(a)>=0||(t[a]=e[a]);return t}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var s=r.createContext({}),p=function(e){var n=r.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},d=function(e){var n=p(e.components);return r.createElement(s.Provider,{value:n},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef((function(e,n){var a=e.components,t=e.mdxType,i=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=p(a),m=t,g=c["".concat(s,".").concat(m)]||c[m]||u[m]||i;return a?r.createElement(g,o(o({ref:n},d),{},{components:a})):r.createElement(g,o({ref:n},d))}));function g(e,n){var a=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var i=a.length,o=new Array(i);o[0]=m;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[c]="string"==typeof e?e:t,o[1]=l;for(var p=2;p<i;p++)o[p]=a[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}m.displayName="MDXCreateElement"},67932:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var r=a(45072),t=(a(11504),a(95788));const i={id:"producing-consuming-data-from-kafka-topics",title:"Tutorial: How to Produce/Consume Data To/From Kafka Topics?",sidebar_label:"Producing/Consuming Data To/From Kafka Topics"},o=void 0,l={unversionedId:"guides/tutorials/producing-consuming-data-from-kafka-topics",id:"guides/tutorials/producing-consuming-data-from-kafka-topics",title:"Tutorial: How to Produce/Consume Data To/From Kafka Topics?",description:"Introduction",source:"@site/docs/guides/tutorials/produce-consume-data-to-from-kafka-topics.md",sourceDirName:"guides/tutorials",slug:"/guides/tutorials/producing-consuming-data-from-kafka-topics",permalink:"/guides/tutorials/producing-consuming-data-from-kafka-topics",draft:!1,editUrl:"https://github.com/zio/zio/edit/series/2.x/docs/guides/tutorials/produce-consume-data-to-from-kafka-topics.md",tags:[],version:"current",frontMatter:{id:"producing-consuming-data-from-kafka-topics",title:"Tutorial: How to Produce/Consume Data To/From Kafka Topics?",sidebar_label:"Producing/Consuming Data To/From Kafka Topics"},sidebar:"guides-sidebar",previous:{title:"Deploying a ZIO Application Using Docker",permalink:"/guides/tutorials/deploy-a-zio-application-using-docker"},next:{title:"Monitoring a ZIO Application Using ZIO's Built-in Metric System",permalink:"/guides/tutorials/monitor-a-zio-application-using-zio-built-in-metric-system"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Running Examples",id:"running-examples",level:2},{value:"Adding Dependencies to The Project",id:"adding-dependencies-to-the-project",level:2},{value:"Setting Up The Kafka Cluster",id:"setting-up-the-kafka-cluster",level:2},{value:"Writing a Simple Producer and Consumer Using ZIO Workflows",id:"writing-a-simple-producer-and-consumer-using-zio-workflows",level:2},{value:"1. Serializing and Deserializing Data",id:"1-serializing-and-deserializing-data",level:3},{value:"2. Creating a Producer",id:"2-creating-a-producer",level:3},{value:"3. Creating a Producer Layer",id:"3-creating-a-producer-layer",level:3},{value:"4. Creating a Consumer",id:"4-creating-a-consumer",level:3},{value:"5. The Complete Example",id:"5-the-complete-example",level:3},{value:"ZIO Kafka With ZIO Streams",id:"zio-kafka-with-zio-streams",level:2},{value:"1. Streaming Producer API",id:"1-streaming-producer-api",level:3},{value:"2. Streaming Consumer API",id:"2-streaming-consumer-api",level:3},{value:"3. Creating a Consumer and Producer Layer",id:"3-creating-a-consumer-and-producer-layer",level:3},{value:"4. The Complete Streaming Example",id:"4-the-complete-streaming-example",level:3},{value:"Producing and Consuming JSON Data",id:"producing-and-consuming-json-data",level:2},{value:"1. Writing Custom Serializer and Deserializer",id:"1-writing-custom-serializer-and-deserializer",level:3},{value:"2. Using the Custom Serde",id:"2-using-the-custom-serde",level:3},{value:"3. The Complete JSON Streaming Example",id:"3-the-complete-json-streaming-example",level:3},{value:"Conclusion",id:"conclusion",level:2}],d={toc:p},c="wrapper";function u(e){let{components:n,...a}=e;return(0,t.yg)(c,(0,r.c)({},d,a,{components:n,mdxType:"MDXLayout"}),(0,t.yg)("h2",{id:"introduction"},"Introduction"),(0,t.yg)("p",null,"Kafka is a distributed, fault-tolerant, message-oriented event-store platform. It is used as a message broker for distributed applications. ZIO Kafka is a library that provides a way to consume and produce data from Kafka topics and it also supports the ability to have streaming consumers and producers."),(0,t.yg)("p",null,"In this tutorial, we will learn how to use ZIO Streams and ZIO Kafka to produce and consume data from Kafka topics."),(0,t.yg)("h2",{id:"running-examples"},"Running Examples"),(0,t.yg)("p",null,"To access the code examples, you can clone the ",(0,t.yg)("a",{parentName:"p",href:"http://github.com/zio/zio-quickstarts"},"ZIO Quickstarts")," project:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"$ git clone git@github.com:zio/zio-quickstarts.git\n$ cd zio-quickstarts/zio-quickstart-kafka\n")),(0,t.yg)("p",null,"And finally, run the application using sbt:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"$ sbt run\n")),(0,t.yg)("h2",{id:"adding-dependencies-to-the-project"},"Adding Dependencies to The Project"),(0,t.yg)("p",null,"In this tutorial, we will be using the following dependencies. So, let's add them to the ",(0,t.yg)("inlineCode",{parentName:"p"},"build.sbt")," file:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},'libraryDependencies += Seq(\n  "dev.zio" %% "zio"         % "2.0.9",\n  "dev.zio" %% "zio-streams" % "2.0.9",\n  "dev.zio" %% "zio-kafka"   % "2.1.1",\n  "dev.zio" %% "zio-json"    % "0.3.0-RC10"\n)\n')),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"ZIO Kafka")," is a ZIO native client for Apache Kafka. It has a high-level streaming API on top of the Java client. So we can produce and consume events using the declarative concurrency model of ZIO Streams.")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"ZIO Stream")," introduces a high-level API for working with streams of values. It is designated to work in a highly concurrent environment. It has seamless integration with ZIO, so we have the ability to use all the features of the ZIO along with the streams, e.g. ",(0,t.yg)("inlineCode",{parentName:"p"},"Scope"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"Schedule"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"ZLayer"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"Queue"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"Hub")," etc. To learn more about ZIO Stream, we have a comprehensive section in on that ",(0,t.yg)("a",{parentName:"p",href:"/reference/stream/"},"here"),".")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"ZIO JSON")," is a library to serialize and deserialize data from/to JSON data type. We will be using this library to serialize and deserialize data when reading and writing JSON data from/to Kafka topics."))),(0,t.yg)("h2",{id:"setting-up-the-kafka-cluster"},"Setting Up The Kafka Cluster"),(0,t.yg)("p",null,"Before we start, we need to set up a Kafka cluster. To set up the kafka cluster for testing purposes we can use the following ",(0,t.yg)("inlineCode",{parentName:"p"},"docker-compose.yml")," file:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-docker-compose"},"version: '2'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - 22181:2181\n  \n  kafka:\n    image: confluentinc/cp-kafka:latest\n    depends_on:\n      - zookeeper\n    ports:\n      - 29092:29092\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n")),(0,t.yg)("p",null,"Now we can run the ",(0,t.yg)("inlineCode",{parentName:"p"},"docker-compose up")," command to start the Kafka cluster:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"$ docker-compose up\n")),(0,t.yg)("p",null,"This will create a Kafka cluster with one instance of Kafka broker and one instance of Zookeeper. Zookeeper is a distributed service that is used to coordinate broker instances inside the cluster."),(0,t.yg)("h2",{id:"writing-a-simple-producer-and-consumer-using-zio-workflows"},"Writing a Simple Producer and Consumer Using ZIO Workflows"),(0,t.yg)("p",null,"To write producers and consumers, using the ZIO Kafka library, we have two choices:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Using ZIO Workflows"),(0,t.yg)("li",{parentName:"ol"},"Using ZIO Streams Workflows")),(0,t.yg)("p",null,"In this section, we will try the first option."),(0,t.yg)("h3",{id:"1-serializing-and-deserializing-data"},"1. Serializing and Deserializing Data"),(0,t.yg)("p",null,"Before we can write a producer and consumer, let's talk about how data is stored in Kafka. Kafka is an event-store platform that stores key-value pairs as raw bytes. So a Kafka broker knows nothing about its records, it just appends the records to its internal log file."),(0,t.yg)("p",null,"So to produce and consume data from Kafka, we need a way to serialize our data to a byte array and deserialize byte arrays to our data types. This is where the ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," data type comes in handy. A ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde[R, T]")," is a serializer and deserializer for values of type ",(0,t.yg)("inlineCode",{parentName:"p"},"T"),", which can use the environment ",(0,t.yg)("inlineCode",{parentName:"p"},"R")," to serialize and deserialize values."),(0,t.yg)("p",null,"Here is the simplified definition of the ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," data type:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"trait Serde[-R, T] {\n  def deserialize(data: Array[Byte]): RIO[R, T]\n  def serialize(value: T)           : RIO[R, Array[Byte]]\n}\n")),(0,t.yg)("p",null,"The companion object of ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," trait contains a set of built-in serializers and deserializers for primitive types:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.long")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.int")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.short")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.float")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.double")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.boolean")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.string")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.byteArray")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.byteBuffer")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"Serde.uuid"))),(0,t.yg)("p",null,"In this example, the type of the ",(0,t.yg)("inlineCode",{parentName:"p"},"key")," is ",(0,t.yg)("inlineCode",{parentName:"p"},"Int")," and the type of the ",(0,t.yg)("inlineCode",{parentName:"p"},"value")," is ",(0,t.yg)("inlineCode",{parentName:"p"},"String"),". So we can use the ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde.int")," for the ",(0,t.yg)("inlineCode",{parentName:"p"},"key")," and the ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde.string")," for the ",(0,t.yg)("inlineCode",{parentName:"p"},"value"),"."),(0,t.yg)("h3",{id:"2-creating-a-producer"},"2. Creating a Producer"),(0,t.yg)("p",null,"ZIO Kafka has several producers that can be used to produce data on Kafka topics. In this example, we will be using the ",(0,t.yg)("inlineCode",{parentName:"p"},"Producer.produce")," method:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"object Producer {\n  def produce[R, K, V](\n    topic: String,\n    key: K,\n    value: V,\n    keySerializer: Serializer[R, K],\n    valueSerializer: Serializer[R, V]\n  ): RIO[R with Producer, RecordMetadata]\n}\n")),(0,t.yg)("p",null,"So let's create a helper function that takes a topic name, key, and value and then returns a ZIO workflow that if we run it, will produce a record to the specified topic:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"def produce(topic: String, key: Long, value: String): RIO[Any with Producer, RecordMetadata] =\n  Producer.produce[Any, Long, String](\n    topic = topic,\n    key = key,\n    value = value,\n    keySerializer = Serde.long,\n    valueSerializer = Serde.string\n  )\n")),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"produce")," function is polymorphic in the type of key and value of the record. Based on what type of key and value we pass, we should provide the appropriate ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," for the key and value."),(0,t.yg)("h3",{id:"3-creating-a-producer-layer"},"3. Creating a Producer Layer"),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"produce")," workflow requires the ",(0,t.yg)("inlineCode",{parentName:"p"},"Producer")," from the ZIO environment. So we need to provide a ",(0,t.yg)("inlineCode",{parentName:"p"},"Producer")," instance to it. So let's create a ",(0,t.yg)("inlineCode",{parentName:"p"},"producer")," layer:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},'import zio._\nimport zio.kafka._\nimport zio.kafka.producer._\n\nval producer: ZLayer[Any, Throwable, Producer] =\n  ZLayer.scoped(\n    Producer.make(\n      ProducerSettings(List("localhost:29092"))\n    )\n  )\n')),(0,t.yg)("p",null,"It is sufficient for this example, although the following helper methods are available for more customization:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"class ProducerSettings {\n  def withBootstrapServers(servers: List[String]): ProducerSettings\n  def withClientId(clientId: String)             : ProducerSettings\n  def withCloseTimeout(duration: Duration)       : ProducerSettings\n  def withProperty(key: String, value: AnyRef)   : ProducerSettings\n  def withProperties(kvs: (String, AnyRef)*)     : ProducerSettings\n  def withProperties(kvs: Map[String, AnyRef])   : ProducerSettings\n}\n")),(0,t.yg)("h3",{id:"4-creating-a-consumer"},"4. Creating a Consumer"),(0,t.yg)("p",null,"ZIO Kafka also has several consumers that can be used to consume data from Kafka topics including the support for ZIO Streams which we will discuss later.  In this example, we will use the ",(0,t.yg)("inlineCode",{parentName:"p"},"Consumer.consumeWith")," function."),(0,t.yg)("p",null,"The following helper function will create a ZIO workflow that if we run it, will run forever and consume records from the given topic and finally print them to the console:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"def consumeAndPrintEvents(groupId: String, topic: String, topics: String*): RIO[Any, Unit] =\n  Consumer.consumeWith(\n    settings = ConsumerSettings(BOOSTRAP_SERVERS).withGroupId(groupId),\n    subscription = Subscription.topics(topic, topics: _*),\n    keyDeserializer = Serde.long,\n    valueDeserializer = Serde.string,\n  )((k, v) => Console.printLine((k, v)).orDie)\n")),(0,t.yg)("h3",{id:"5-the-complete-example"},"5. The Complete Example"),(0,t.yg)("p",null,"Now it's time to combine all the above steps to create a ZIO workflow that will produce and consume data from the Kafka cluster:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},'import org.apache.kafka.clients.producer.RecordMetadata\nimport zio._\nimport zio.kafka.consumer._\nimport zio.kafka.producer.{Producer, ProducerSettings}\nimport zio.kafka.serde._\n\nobject SimpleApp extends ZIOAppDefault {\n  private val BOOSTRAP_SERVERS = List("localhost:29092")\n  private val KAFKA_TOPIC = "hello"\n\n  private def produce(\n      topic: String,\n      key: Long,\n      value: String\n  ): RIO[Any with Producer, RecordMetadata] =\n    Producer.produce[Any, Long, String](\n      topic = topic,\n      key = key,\n      value = value,\n      keySerializer = Serde.long,\n      valueSerializer = Serde.string\n    )\n\n  private def consumeAndPrintEvents(\n      groupId: String,\n      topic: String,\n      topics: String*\n  ): RIO[Any, Unit] =\n    Consumer.consumeWith(\n      settings = ConsumerSettings(BOOSTRAP_SERVERS)\n        .withGroupId(groupId),\n      subscription = Subscription.topics(topic, topics: _*),\n      keyDeserializer = Serde.long,\n      valueDeserializer = Serde.string\n    )(record => Console.printLine((record.key(), record.value())).orDie)\n\n  private val producer: ZLayer[Any, Throwable, Producer] =\n    ZLayer.scoped(\n      Producer.make(\n        ProducerSettings(BOOSTRAP_SERVERS)\n      )\n    )\n\n  def run =\n    for {\n      f <- consumeAndPrintEvents("my-consumer-group", KAFKA_TOPIC).fork\n      _ <-\n        Clock.currentDateTime\n          .flatMap { time =>\n            produce(KAFKA_TOPIC, time.getHour, s"$time -- Hello, World!")\n          }\n          .schedule(Schedule.spaced(1.second))\n          .provide(producer)\n      _ <- f.join\n    } yield ()\n\n}\n')),(0,t.yg)("h2",{id:"zio-kafka-with-zio-streams"},"ZIO Kafka With ZIO Streams"),(0,t.yg)("p",null,"As we said before, to write producers and consumers using the ZIO Kafka library, we have two choices:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Using ZIO Workflows"),(0,t.yg)("li",{parentName:"ol"},"Using ZIO Streams Workflows")),(0,t.yg)("p",null,"In the previous section, we used the ZIO Kafka with the ZIO Workflow. The ZIO Kafka also works with the ZIO Streams seamlessly. So instead of using the ",(0,t.yg)("inlineCode",{parentName:"p"},"Producer.produce")," and ",(0,t.yg)("inlineCode",{parentName:"p"},"Consumer.consumeWith")," functions, we can use the streaming APIs provided by ZIO Kafka."),(0,t.yg)("h3",{id:"1-streaming-producer-api"},"1. Streaming Producer API"),(0,t.yg)("p",null,"To produce data using ZIO Streams, ZIO Kafka has a ",(0,t.yg)("inlineCode",{parentName:"p"},"Producer.produceAll")," API, which is a ",(0,t.yg)("inlineCode",{parentName:"p"},"ZPipeline"),". It takes streams of ",(0,t.yg)("inlineCode",{parentName:"p"},"ProducerRecord[K, V]")," as upstream and uses the ",(0,t.yg)("inlineCode",{parentName:"p"},"Producer")," from the environment to produce streams to the Kafka topic and then returns a stream of ",(0,t.yg)("inlineCode",{parentName:"p"},"RecordMetadata")," as downstream:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"object Producer {\n  def produceAll[R, K, V](\n    keySerializer: Serializer[R, K],\n    valueSerializer: Serializer[R, V]\n  ): ZPipeline[R with Producer, Throwable, ProducerRecord[K, V], RecordMetadata] = ???\n}\n")),(0,t.yg)("p",null,"Note that the ",(0,t.yg)("inlineCode",{parentName:"p"},"ZStream")," implicitly chunks the records into batches for the sake of performance. So the ",(0,t.yg)("inlineCode",{parentName:"p"},"produceAll")," produces records in batches instead of one at a time."),(0,t.yg)("h3",{id:"2-streaming-consumer-api"},"2. Streaming Consumer API"),(0,t.yg)("p",null,"Creating a streaming consumer is also simple. We can use the ",(0,t.yg)("inlineCode",{parentName:"p"},"Consumer.plainStream")," API to create a ",(0,t.yg)("inlineCode",{parentName:"p"},"ZStream")," that if we run it, will use the ",(0,t.yg)("inlineCode",{parentName:"p"},"Consumer")," from the environment to consume records from a Kafka topic and then returns a stream of ",(0,t.yg)("inlineCode",{parentName:"p"},"CommittableRecord[K, V]")," as downstream:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"object Consumer {\n  def plainStream[R, K, V](\n    keyDeserializer: Deserializer[R, K],\n    valueDeserializer: Deserializer[R, V],\n    bufferSize: Int = 4\n  ): ZStream[R with Consumer, Throwable, CommittableRecord[K, V]] = ???\n}\n")),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"CommittableRecord")," is a record that can be committed to Kafka via ",(0,t.yg)("inlineCode",{parentName:"p"},"CommittableRecord#commit")," to indicate that the record has been consumed successfully. After we commit the record that we have consumed, if our application crashes, when we restart, we can resume consuming from the last record that we have committed."),(0,t.yg)("p",null,"For example, if we want to consume records and then save them to a file system, we can run the ",(0,t.yg)("inlineCode",{parentName:"p"},"CommittableRecord#commit")," function after we wrote the record to the file system. So we are sure that the record has been persisted in the file system:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},'import zio._\nimport zio.stream._\nimport zio.kafka._\nimport zio.kafka.consumer._\nimport zio.kafka.serde._\n\nval KAFKA_TOPIC = "my-topic"\n\nval c: ZStream[Consumer, Throwable, Nothing] =\n  Consumer\n    .plainStream(Subscription.topics(KAFKA_TOPIC), Serde.int, Serde.string)\n    .tap(e => Console.printLine(e.value))\n    .map(_.offset)\n    .mapZIO(_.commit)\n    .drain\n')),(0,t.yg)("p",null,"The problem with this approach is that we are committing offsets for each record that we consume. This will cause a lot of overhead and will slow down the consumption of the records. To avoid this, we can aggregate the offsets into batches and commit them all at once. This can be done by using the ",(0,t.yg)("inlineCode",{parentName:"p"},"ZStream#aggregateAsync")," along with the ",(0,t.yg)("inlineCode",{parentName:"p"},"Consumer.offsetBatches")," sink:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},'import zio._\nimport zio.stream._\nimport zio.kafka._\nimport zio.kafka.consumer._\nimport zio.kafka.serde._\n\nval KAFKA_TOPIC = "my-topic"\n\nval c: ZStream[Consumer, Throwable, Nothing] =\n  Consumer\n    .plainStream(Subscription.topics(KAFKA_TOPIC), Serde.int, Serde.string)\n    .tap(e => Console.printLine(e.value))\n    .map(_.offset)\n    .aggregateAsync(Consumer.offsetBatches)\n    .mapZIO(_.commit)\n    .drain\n')),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"Consumer.offsetBatches")," sink folds ",(0,t.yg)("inlineCode",{parentName:"p"},"Offset"),"s into ",(0,t.yg)("inlineCode",{parentName:"p"},"OffsetBatch")," which contains the maximum offset we have seen so far. So instead of committing the offset for each record, we commit the maximum offset of all the records in the batch."),(0,t.yg)("h3",{id:"3-creating-a-consumer-and-producer-layer"},"3. Creating a Consumer and Producer Layer"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},'import zio._\nimport zio.stream._\nimport zio.kafka._\nimport zio.kafka.producer._\nimport zio.kafka.consumer._\n\nval BOOSTRAP_SERVERS = List("localhost:29092")\n\nprivate val producer: ZLayer[Any, Throwable, Producer] =\n  ZLayer.scoped(\n    Producer.make(\n      ProducerSettings(BOOSTRAP_SERVERS)\n    )\n  )\n\nprivate val consumer: ZLayer[Any, Throwable, Consumer] =\n  ZLayer.scoped(\n    Consumer.make(\n      ConsumerSettings(BOOSTRAP_SERVERS)\n        .withGroupId("streaming-kafka-app")\n    )\n  )\n')),(0,t.yg)("h3",{id:"4-the-complete-streaming-example"},"4. The Complete Streaming Example"),(0,t.yg)("p",null,"It's time to create a full working example of ZIO Kafka with ZIO streams:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},'import org.apache.kafka.clients.producer.ProducerRecord\nimport zio._\nimport zio.kafka.consumer._\nimport zio.kafka.producer.{Producer, ProducerSettings}\nimport zio.kafka.serde._\nimport zio.stream.ZStream\n\nobject StreamingKafkaApp extends ZIOAppDefault {\n  private val BOOSTRAP_SERVERS = List("localhost:29092")\n  private val KAFKA_TOPIC      = "streaming-hello"\n\n  private val producer: ZLayer[Any, Throwable, Producer] =\n    ZLayer.scoped(\n      Producer.make(\n        ProducerSettings(BOOSTRAP_SERVERS)\n      )\n    )\n\n  private val consumer: ZLayer[Any, Throwable, Consumer] =\n    ZLayer.scoped(\n      Consumer.make(\n        ConsumerSettings(BOOSTRAP_SERVERS)\n          .withGroupId("streaming-kafka-app")\n      )\n    )\n\n  def run = {\n    val p: ZStream[Producer, Throwable, Nothing] =\n      ZStream\n        .repeatZIO(Clock.currentDateTime)\n        .schedule(Schedule.spaced(1.second))\n        .map(time => new ProducerRecord(KAFKA_TOPIC, time.getMinute, s"$time -- Hello, World!"))\n        .via(Producer.produceAll(Serde.int, Serde.string))\n        .drain\n        \n    val c: ZStream[Consumer, Throwable, Nothing] =\n      Consumer\n        .plainStream(Subscription.topics(KAFKA_TOPIC), Serde.int, Serde.string)\n        .tap(e => Console.printLine(e.value))\n        .map(_.offset)\n        .aggregateAsync(Consumer.offsetBatches)\n        .mapZIO(_.commit)\n        .drain\n    \n    (p merge c).runDrain.provide(producer, consumer)\n  }\n\n}\n')),(0,t.yg)("h2",{id:"producing-and-consuming-json-data"},"Producing and Consuming JSON Data"),(0,t.yg)("p",null,"Until now, we learned how to work with simple primitive types like ",(0,t.yg)("inlineCode",{parentName:"p"},"Int"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"String"),", etc and how to use their ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," instances to encode and decode the data."),(0,t.yg)("p",null,"In this section, we are going to learn how to work with user-defined data types (like case classes e.g. ",(0,t.yg)("inlineCode",{parentName:"p"},"Event"),"), and how to produce and consume the JSON data representing our user-defined data types. We also will learn how to use the ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," built-in instances to create more complex ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," instances."),(0,t.yg)("h3",{id:"1-writing-custom-serializer-and-deserializer"},"1. Writing Custom Serializer and Deserializer"),(0,t.yg)("p",null,"In ZIO Kafka all of the built-in serializers/deserializers are instances of the ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," trait, which has two useful methods:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"trait Serde[-R, T] extends Deserializer[R, T] with Serializer[R, T] {\n  def inmap[U](f: T => U)(g: U => T): Serde[R, U] =\n    Serde(map(f))(contramap(g))\n\n  def inmapM[R1 <: R, U](f: T => RIO[R1, U])(g: U => RIO[R1, T]): Serde[R1, U] =\n    Serde(mapM(f))(contramapM(g))\n}\n")),(0,t.yg)("p",null,"Using the ",(0,t.yg)("inlineCode",{parentName:"p"},"inmap")," and ",(0,t.yg)("inlineCode",{parentName:"p"},"inmapM")," combinators, we can create our own serializers and deserializers on top of the built-in ones:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"The ",(0,t.yg)("inlineCode",{parentName:"li"},"inmap")," is used to transform the ",(0,t.yg)("inlineCode",{parentName:"li"},"Serde")," type ",(0,t.yg)("inlineCode",{parentName:"li"},"U")," with pure transformations of ",(0,t.yg)("inlineCode",{parentName:"li"},"f")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"g"),"."),(0,t.yg)("li",{parentName:"ul"},"The ",(0,t.yg)("inlineCode",{parentName:"li"},"inmapM")," is used to transform the ",(0,t.yg)("inlineCode",{parentName:"li"},"Serde")," type ",(0,t.yg)("inlineCode",{parentName:"li"},"U")," with effectful transformations of ",(0,t.yg)("inlineCode",{parentName:"li"},"f")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"g"),". As it accepts effectful transformations, we can encode any parsing failure with a ",(0,t.yg)("inlineCode",{parentName:"li"},"ZIO")," workflow.")),(0,t.yg)("p",null,"Let's say we have a case class ",(0,t.yg)("inlineCode",{parentName:"p"},"Event")," with the following fields:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"import java.time.OffsetDateTime\nimport java.util.UUID\n\ncase class Event(\n  uuid: UUID,\n  timestamp: OffsetDateTime,\n  message: String\n)\n")),(0,t.yg)("p",null,"First, we need to define a JSON decoder and encoder for it:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"import zio.json._\n\nobject Event {\n  implicit val encoder: JsonEncoder[Event] =\n    DeriveJsonEncoder.gen[Event]\n\n  implicit val decoder: JsonDecoder[Event] =\n    DeriveJsonDecoder.gen[Event]\n}\n")),(0,t.yg)("p",null,"Then we need to create a ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," for the ",(0,t.yg)("inlineCode",{parentName:"p"},"Event")," type. To convert ",(0,t.yg)("inlineCode",{parentName:"p"},"Event")," to JSON and back, we will use the ZIO JSON library, and to define ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," for the ",(0,t.yg)("inlineCode",{parentName:"p"},"Event")," type, we will use the ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde#inmapM")," combinator:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},"import zio._\nimport zio.kafka.serde._\n\nobject KafkaSerde {\n  val key: Serde[Any, Int] =\n    Serde.int\n\n  val value: Serde[Any, Event] =\n    Serde.string.inmapM[Any, Event](s =>\n      ZIO.fromEither(s.fromJson[Event])\n        .mapError(e => new RuntimeException(e))\n    )(r => ZIO.succeed(r.toJson))\n}\n")),(0,t.yg)("p",null,"As we can see, we use the ",(0,t.yg)("inlineCode",{parentName:"p"},"String#fromJson")," to convert the string to an ",(0,t.yg)("inlineCode",{parentName:"p"},"Event")," object and we also encode any parsing failure with a ",(0,t.yg)("inlineCode",{parentName:"p"},"RuntimeException")," in the ",(0,t.yg)("inlineCode",{parentName:"p"},"ZIO")," workflow."),(0,t.yg)("h3",{id:"2-using-the-custom-serde"},"2. Using the Custom Serde"),(0,t.yg)("p",null,"After we have defined our custom ",(0,t.yg)("inlineCode",{parentName:"p"},"Serde")," for the ",(0,t.yg)("inlineCode",{parentName:"p"},"Event")," type, we can use it in our Kafka producer and consumer streams:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},'import zio._\nimport zio.stream._\nimport zio.kafka.serde._\nimport zio.kafka.producer._\nimport zio.kafka.consumer._\nimport org.apache.kafka.clients.producer.ProducerRecord\n\nval KAFKA_TOPIC = "json-streaming-hello"\n\nval events: UStream[ProducerRecord[Int, Event]] = ???\n\nval producer = \n  events.via(Producer.produceAll(KafkaSerde.key, KafkaSerde.value))\n\nval consumer =\n  Consumer\n    .plainStream(Subscription.topics(KAFKA_TOPIC), KafkaSerde.key, KafkaSerde.value)\n')),(0,t.yg)("h3",{id:"3-the-complete-json-streaming-example"},"3. The Complete JSON Streaming Example"),(0,t.yg)("p",null,"Here is a full working example of producing and consuming JSON data with ZIO Kafka, ZIO Streams and ZIO JSON:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-scala"},'import org.apache.kafka.clients.producer.ProducerRecord\nimport zio._\nimport zio.json._\nimport zio.kafka.consumer._\nimport zio.kafka.producer.{Producer, ProducerSettings}\nimport zio.kafka.serde._\nimport zio.stream.ZStream\n\nimport java.time.OffsetDateTime\nimport java.util.UUID\n\ncase class Event(uuid: UUID, timestamp: OffsetDateTime, message: String)\n\nobject Event {\n  implicit val encoder: JsonEncoder[Event] =\n    DeriveJsonEncoder.gen[Event]\n\n  implicit val decoder: JsonDecoder[Event] =\n    DeriveJsonDecoder.gen[Event]\n}\n\nobject KafkaSerde {\n  val key: Serde[Any, Int] =\n    Serde.int\n\n  val value: Serde[Any, Event] =\n    Serde.string.inmapM[Any, Event](s =>\n      ZIO.fromEither(s.fromJson[Event])\n        .mapError(e => new RuntimeException(e))\n    )(r => ZIO.succeed(r.toJson))\n}\n\nobject JsonStreamingKafkaApp extends ZIOAppDefault {\n  private val BOOSTRAP_SERVERS = List("localhost:29092")\n  private val KAFKA_TOPIC      = "json-streaming-hello"\n\n  private val producer: ZLayer[Any, Throwable, Producer] =\n    ZLayer.scoped(\n      Producer.make(\n        ProducerSettings(BOOSTRAP_SERVERS)\n      )\n    )\n\n  private val consumer: ZLayer[Any, Throwable, Consumer] =\n    ZLayer.scoped(\n      Consumer.make(\n        ConsumerSettings(BOOSTRAP_SERVERS)\n          .withGroupId("streaming-kafka-app")\n      )\n    )\n\n  def run = {\n    val p: ZStream[Producer, Throwable, Nothing] =\n      ZStream\n        .repeatZIO(Random.nextUUID <*> Clock.currentDateTime)\n        .schedule(Schedule.spaced(1.second))\n        .map { case (uuid, time) =>\n          new ProducerRecord(\n            KAFKA_TOPIC,\n            time.getMinute,\n            Event(uuid, time, "Hello, World!")\n          )\n        }\n        .via(Producer.produceAll(KafkaSerde.key, KafkaSerde.value))\n        .drain\n\n    val c: ZStream[Consumer, Throwable, Nothing] =\n      Consumer\n        .plainStream(Subscription.topics(KAFKA_TOPIC), KafkaSerde.key, KafkaSerde.value)\n        .tap(e => Console.printLine(e.value))\n        .map(_.offset)\n        .aggregateAsync(Consumer.offsetBatches)\n        .mapZIO(_.commit)\n        .drain\n\n    (p merge c).runDrain.provide(producer, consumer)\n  }\n\n}\n')),(0,t.yg)("h2",{id:"conclusion"},"Conclusion"),(0,t.yg)("p",null,"In this tutorial first, we learned how to create a producer and consumer for Kafka using the ZIO workflow with ZIO Kafka. Then we learned how to do the same with ZIO Streams. We also learned how to create a custom serializer and deserializer for the Kafka records and how to produce and consume JSON data using the ZIO JSON library."),(0,t.yg)("p",null,"All the source code associated with this article is available on the ",(0,t.yg)("a",{parentName:"p",href:"http://github.com/zio/zio-quickstarts"},"ZIO Quickstart")," project on Github."))}u.isMDXComponent=!0}}]);